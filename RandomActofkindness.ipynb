{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# General libraries.\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV #update module model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GMM\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This project aims to predict whether a reddit post asking for pizzas would get funded. Since it's a binary classification problem, we will explore several algorithms with a focus on logistic regression. In particular, we will look into details how to extract features from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Pre-Processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data in its raw form consists of 4040 observations of 31 features.  The original columns consist of 19 integer values, 4 floats, and 8 objects (there is one boolean column which is the outcome variable).  In order to extract predictive value from the dataset a good deal of pre-processing and feature engineering was required.  A walkthrough of the various steps taken follows below in a narrative format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected character found when decoding 'true'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b27f8f43bdf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load json training data into pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tedpham/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/pandas/io/json/json.pyc\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines)\u001b[0m\n\u001b[1;32m    352\u001b[0m         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n\u001b[1;32m    353\u001b[0m                           \u001b[0mkeep_default_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                           date_unit).parse()\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tedpham/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/pandas/io/json/json.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tedpham/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/pandas/io/json/json.pyc\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 639\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             decoded = dict((str(k), v)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected character found when decoding 'true'"
     ]
    }
   ],
   "source": [
    "#load json training data into pandas dataframe\n",
    "df = pd.read_json('train.json')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll start by transforming the outcome variable into a binary variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3046\n",
       "1     994\n",
       "Name: requester_received_pizza, dtype: int64"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['requester_received_pizza'] = np.where(df['requester_received_pizza'] == True, 1, 0)\n",
    "df['requester_received_pizza'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll remove all the \"_at_retrieval\" columns from the dataset as they are not found in the test data set and therefore represent data that is not avaialable at the time of the request for pizza.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_indexes = []\n",
    "for i, name in enumerate(df.columns):\n",
    "    if re.findall('retrieval', name):\n",
    "        pass\n",
    "    else:\n",
    "        good_indexes.append(i)\n",
    "\n",
    "# Remove at_retrieval fields from dataframce df\n",
    "columns = df.columns[good_indexes]\n",
    "df =  df.loc[:,columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also found that there were several other columns that were not needed for predictive power, and we therefore removed them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4040 entries, 0 to 4039\n",
      "Data columns (total 15 columns):\n",
      "request_text                                          4040 non-null object\n",
      "request_title                                         4040 non-null object\n",
      "requester_account_age_in_days_at_request              4040 non-null float64\n",
      "requester_days_since_first_post_on_raop_at_request    4040 non-null float64\n",
      "requester_number_of_comments_at_request               4040 non-null int64\n",
      "requester_number_of_comments_in_raop_at_request       4040 non-null int64\n",
      "requester_number_of_posts_at_request                  4040 non-null int64\n",
      "requester_number_of_posts_on_raop_at_request          4040 non-null int64\n",
      "requester_number_of_subreddits_at_request             4040 non-null int64\n",
      "requester_received_pizza                              4040 non-null int64\n",
      "requester_subreddits_at_request                       4040 non-null object\n",
      "requester_upvotes_minus_downvotes_at_request          4040 non-null int64\n",
      "requester_upvotes_plus_downvotes_at_request           4040 non-null int64\n",
      "unix_timestamp_of_request                             4040 non-null int64\n",
      "unix_timestamp_of_request_utc                         4040 non-null int64\n",
      "dtypes: float64(2), int64(10), object(3)\n",
      "memory usage: 505.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Drop six more columns from dataset\n",
    "df.drop(['giver_username_if_known', 'post_was_edited', 'request_id', 'request_text_edit_aware', \n",
    "        'requester_user_flair', 'requester_username'], axis=1, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the \"at_retrieval\" columns and the other six columns, reduces our dataset by 17 total features (leaving us with 14 features, not including the outcome variable ).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### During the EDA phase of this project we found that several (104 to be exact), observations had a \"request_text\" length of zero, some of these observations actually ended up being given a pizza.  After looking through the data, we discovered that some people had left their request in the \"request_title\" field of the RAOP Reddit page and had left their request_text field blank.  In order to clean this discrepancy up, we decided to combine these two fields together, as it is unclear if the benefactors (those who ended up giving pizzas away),  were responding to the '\"request_title\" field, the \"request_text\" field, or both when they made their altruistic decision.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB0AAAAPBAMAAADqo9msAAAAMFBMVEX///8AAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAVO8Qq5l2zWaJMt0i\nu0SCRuA9AAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAoElEQVQIHWNgAAHmyM4FQIorAEgwKjMwsAsw\n7wEyuS8wMJiEfGZgaGJgmA7kJwH5DGxA/hUGBnkDBp52GP8LA8N7AQZOdiif+SuQn8DQA+Pz/GRg\nWD+BOQHOB8qvn8DJAOND1EvB+QxA8+SFJyD4dxkY+g1dXPxVEiD2A90TDnQKE8x+bgFmLSCfH8hn\ndfruwcA8LWUBAwPn/S8NQFEUAACg3zHkwwujqAAAAABJRU5ErkJggg==\n",
      "text/latex": [
       "$$104$$"
      ],
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show that 104 observations have a blank \"request_text\" field\n",
    "len(df[df['request_text'].str.len() == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    colorado springs help us please hi i am in nee...\n",
      "1    california, no cash and i could use some dinne...\n",
      "2    hungry couple in dundee, scotland would love s...\n",
      "3    in canada (ontario), just got home from school...\n",
      "4    old friend coming to visit. would love to feed...\n",
      "Name: request_text_n_title, dtype: object\n",
      "\n",
      "After combining request_title and request_text, number of requests with length of \"zero\": 0\n"
     ]
    }
   ],
   "source": [
    "#1. Combine request_text and request_title fields\n",
    "#2. Lowercase all words \n",
    "\n",
    "df['request_text_n_title'] = (df['request_title'] + ' ' + df['request_text'])\n",
    "df['request_text_n_title'] = [ text.split(\" \",1)[1].lower() for text in df['request_text_n_title']]\n",
    "print df['request_text_n_title'].head()\n",
    " \n",
    "#3. Add a total length feature to the dataset \n",
    "df['total_length'] =df['request_text_n_title'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "#4. Ensure there are no zero length requests in the new feature/column\n",
    "print '\\nAfter combining request_title and request_text, number of requests with length of \\\n",
    "\"zero\": {}'.format(len(df[df['request_text_n_title'] == 0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_text_n_title</th>\n",
       "      <th>total_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>colorado springs help us please hi i am in nee...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california, no cash and i could use some dinne...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hungry couple in dundee, scotland would love s...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in canada (ontario), just got home from school...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>old friend coming to visit. would love to feed...</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i'll give a two week xbox live code for a slic...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                request_text_n_title  total_length\n",
       "0  colorado springs help us please hi i am in nee...            72\n",
       "1  california, no cash and i could use some dinne...            25\n",
       "2  hungry couple in dundee, scotland would love s...            68\n",
       "3  in canada (ontario), just got home from school...            39\n",
       "4  old friend coming to visit. would love to feed...           115\n",
       "5  i'll give a two week xbox live code for a slic...            47"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showcase of new added features to dataset\n",
    "df.loc[:5,['request_text_n_title', 'total_length']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Baseline Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With pre-processing out of the way, we decided to run a baseline text analysis model using the \"request_text_n_title\" feature alone.  This model serves as our basis against which we can compare the success or failure of all futue feature engineering efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIIa. Train and Dev set creation\n",
    "#### We begin by forming our training and dev sets.  Given that the success rate for receiving a pizza in the original dataset is ~25%, we'll check our randomized training and dev sets to ensure a similar success rate is preserved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original success rate: 0.246\n",
      "Training success rate: 0.2462\n",
      "Dev success rate: 0.2452\n",
      "\n",
      "\n",
      "Training Data shape: \t(3200,)\n",
      "Training Labels shape: \t(3200,)\n",
      "Dev data shape: \t(840,)\n",
      "Dev Labels shape: \t(840,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#separate into features and labels\n",
    "text_features = df['request_text_n_title'].values # text features\n",
    "success_rate = sum(df['requester_received_pizza'])/4040.\n",
    "print \"Original success rate: {}\".format(round(success_rate, 4))\n",
    "\n",
    "# Create target field for received pizza are only 1's and 0's\n",
    "target = df['requester_received_pizza'].values\n",
    "\n",
    "#shuffle our data to ensure randomization\n",
    "shuffle = np.random.permutation(len(text_features))\n",
    "text_features, target = text_features[shuffle], target[shuffle]\n",
    "\n",
    "#separate into training and dev groups\n",
    "train_data, train_labels = text_features[:3200], target[:3200]\n",
    "dev_data, dev_labels = text_features[3200:], target[3200:]\n",
    "\n",
    "#check to ensure success rate is roughly preserved across sets \n",
    "train_success_rate = sum(train_labels)/3200.\n",
    "dev_success_rate = sum(dev_labels)/840.\n",
    "print \"Training success rate: {}\".format(round(train_success_rate, 4))\n",
    "print \"Dev success rate: {}\".format(round(dev_success_rate, 4))\n",
    "\n",
    "#check to ensure we've got the right datasets\n",
    "print '\\n\\nTraining Data shape: \\t{}'.format(train_data.shape)\n",
    "print 'Training Labels shape: \\t{}'.format(train_labels.shape)\n",
    "print 'Dev data shape: \\t{}'.format(dev_data.shape)\n",
    "print 'Dev Labels shape: \\t{}'.format(dev_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIIb. Logistic Regression\n",
    "#### Our initial model will make use of the sklearn TfidfVectorizer function to encapsulate predictive power from the training data.  After some initial trial runs, we realized that we needed to limit the number of features to make a useful model.  In this case, a predictive \"sweet spot\" emerged in the 350+ - 400 range, so we limited our feature number to this amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE LOGISTIC REGRESSION\n",
      "----------------------------\n",
      "Baseline ROC AUC score: 0.5953\n",
      "\n",
      "\n",
      "RESTRICTED FEATURES LOGISTIC REGRESSION\n",
      "---------------------------------------\n",
      "Max number of features: 387\n",
      "Best ROC AUC Score: 0.6159\n",
      "\n",
      "\n",
      "LOGISTIC REGRESSION with tuning c value and restricted # of features\n",
      "--------------------------------------------------------------------\n",
      "Best C-value: 2.14655345739\n",
      "Best ROC AUC Score: 0.628\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fit logistic classifier to training data\n",
    "print \"BASELINE LOGISTIC REGRESSION\"\n",
    "print \"----------------------------\"\n",
    "vec = TfidfVectorizer()\n",
    "train_matrix = vec.fit_transform(train_data)\n",
    "dev_matrix = vec.transform(dev_data)\n",
    "lr= LogisticRegression(n_jobs=-1, class_weight='balanced').fit(train_matrix, train_labels)\n",
    "predictions = lr.predict(dev_matrix)\n",
    "score = round(roc_auc_score(dev_labels, predictions, average='weighted'), 4)\n",
    "print \"Baseline ROC AUC score: {}\".format(score)\n",
    "\n",
    "print \"\\n\\nRESTRICTED FEATURES LOGISTIC REGRESSION\"\n",
    "print \"---------------------------------------\"\n",
    "\n",
    "model_scores = []\n",
    "range_features = np.arange(385,400)\n",
    "\n",
    "for features in range_features:    \n",
    "    vec = TfidfVectorizer(stop_words='english',sublinear_tf=1, ngram_range=(1,1),max_features=features)\n",
    "    train_matrix = vec.fit_transform(train_data)\n",
    "    dev_matrix = vec.transform(dev_data)\n",
    "    lr= LogisticRegression(n_jobs=-1, class_weight='balanced').fit(train_matrix, train_labels)\n",
    "    predictions = lr.predict(dev_matrix)\n",
    "    model_scores.append(round(metrics.roc_auc_score(dev_labels, predictions, average = 'weighted'), 4))\n",
    "    best_score = round(max(model_scores), 4)\n",
    "print \"Max number of features: {}\".format(range_features[np.argmax(model_scores)])\n",
    "print \"Best ROC AUC Score: {}\".format(best_score)\n",
    "\n",
    "#best_max_feature\n",
    "best_max_feature = range_features[np.argmax(model_scores)]\n",
    "\n",
    "#fit logistic classifier to training data\n",
    "vec = TfidfVectorizer(stop_words='english',sublinear_tf=1, ngram_range=(1,1), max_features=best_max_feature)\n",
    "train_matrix = vec.fit_transform(train_data)\n",
    "dev_matrix = vec.transform(dev_data)\n",
    "\n",
    "print \"\\n\"\n",
    "print \"LOGISTIC REGRESSION with tuning c value and restricted # of features\"\n",
    "print \"--------------------------------------------------------------------\"\n",
    "c_values = np.logspace(.0001, 2, 200)\n",
    "c_scores = []\n",
    "for value in c_values:\n",
    "    lr = LogisticRegression(C=value, n_jobs=-1, class_weight='balanced', penalty='l2')\n",
    "    lr.fit(train_matrix, train_labels)\n",
    "    predictions = lr.predict(dev_matrix)\n",
    "    c_scores.append(round(metrics.roc_auc_score(dev_labels, predictions, average = 'weighted'), 4))\n",
    "    \n",
    "best_c_value = c_values[np.argmax(c_scores)]\n",
    "print \"Best C-value: {}\".format(best_c_value)\n",
    "print \"Best ROC AUC Score: {}\".format(c_scores[np.argmax(c_scores)])\n",
    "lr = LogisticRegression(C=best_c_value, n_jobs=-1).fit(train_matrix, train_labels)\n",
    "predictions = lr.predict(dev_matrix)\n",
    "print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our initial baseline model with Logistic Regression does suprisingly well using only the raw text without any feature engineering or hyperparameter tuning. The initial score represents a rough 9% improvement over random guessing alone.  After restricting the number of features in the model we see a 2% improvement in predictive accuracy, suggesting that the great majority of unique words in the feature space do not contribute to predictive accuracy.   We further tested baseline models using Multinomial Naive Bayes and SVM.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIIc. Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERNOULLI NAIVE BAYES\n",
      "-----------------------\n",
      "Best ROC AUC Score: 0.6125\n",
      "Max Features: 425\n"
     ]
    }
   ],
   "source": [
    "#fit Multinomial Naive Bayes classifier to training data\n",
    "print \"BERNOULLI NAIVE BAYES\"\n",
    "print \"-----------------------\"\n",
    "model_scores = []\n",
    "range_features = np.arange(415, 431)\n",
    "for features in range_features:\n",
    "    vec = TfidfVectorizer(max_features=features)\n",
    "    train_matrix = vec.fit_transform(train_data)\n",
    "    dev_matrix = vec.transform(dev_data)\n",
    "    alphas = np.linspace(.0001, 1, 20)\n",
    "    bnb = BernoulliNB(alpha=.001).fit(train_matrix, train_labels)\n",
    "    predictions = bnb.predict(dev_matrix)\n",
    "    model_scores.append(round(roc_auc_score(dev_labels, predictions, average='weighted'), 4))\n",
    "print \"Best ROC AUC Score: {}\".format(max(model_scores))\n",
    "print \"Max Features: {}\".format(range_features[np.argmax(model_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Bernoulli NB we get similar results to LR, again making use of restricting the number of features in our training matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIId. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support Vector Machine\n",
      "------------------------\n",
      "0.5309\n"
     ]
    }
   ],
   "source": [
    "print \"\\nSupport Vector Machine\"\n",
    "print \"------------------------\"\n",
    "vec = TfidfVectorizer(max_features=best_max_feature, stop_words='english')\n",
    "train_matrix = vec.fit_transform(train_data)\n",
    "dev_matrix = vec.transform(dev_data)\n",
    "svc = LinearSVC().fit(train_matrix, train_labels)\n",
    "predictions = svc.predict(dev_matrix)\n",
    "print round(roc_auc_score(dev_labels, predictions, average='weighted'),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM performs no better than previous models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting aside text processing for the time being, we decided to focus on specific features within the dataset that could possibly lead to increased predictive power.  For example, we noted that users who included pictues in their request (presumably as a means of validating their story), were disporportionately more likely to receive a pizza.  Similarly, users who wrote longer requests, or who had spent more time in the RAOP community were also more likely to recieve a pizza.  Therefore, what follows is an explanation of our attempts at engineering features to build the best predictive model possible.  With a few exceptions all features were binarized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Including an image\n",
    "#### The inclusion of an image in the request indicates that the requester is providing some evidence of their need for a pizza. A photo might be a screenshot of a bank account balance, a large bill, an injury, or other misfortune. Adding such evidence increases the odds of receiving a pizza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create feature where in image is included in the request\n",
    "df['image_incl'] = np.where(df['request_text_n_title'].str.contains(\"imgur\"), int(1), int(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Community standing/status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winners</th>\n",
       "      <th>losers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>994.000000</td>\n",
       "      <td>3046.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1848.437240</td>\n",
       "      <td>1501.113257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5598.137867</td>\n",
       "      <td>3231.267421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-10.895197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>132.845203</td>\n",
       "      <td>5.020179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>666.343900</td>\n",
       "      <td>480.832176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2127.290081</td>\n",
       "      <td>1762.444358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>156272.652604</td>\n",
       "      <td>89490.103021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             winners        losers\n",
       "count     994.000000   3046.000000\n",
       "mean     1848.437240   1501.113257\n",
       "std      5598.137867   3231.267421\n",
       "min         0.000000    -10.895197\n",
       "25%       132.845203      5.020179\n",
       "50%       666.343900    480.832176\n",
       "75%      2127.290081   1762.444358\n",
       "max    156272.652604  89490.103021"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create feature that is an aggregate of all indicators of community status including seniority\n",
    "\n",
    "df['karma'] = df['requester_account_age_in_days_at_request'] + df['requester_days_since_first_post_on_raop_at_request']\\\n",
    "+ df['requester_number_of_comments_at_request'] + df['requester_number_of_comments_in_raop_at_request'] + \\\n",
    "df['requester_number_of_posts_at_request'] + df['requester_number_of_posts_on_raop_at_request'] + \\\n",
    "df['requester_number_of_subreddits_at_request'] + df['requester_upvotes_minus_downvotes_at_request']\n",
    "\n",
    "karma_winners = df['karma'][df['requester_received_pizza'] == 1].describe()\n",
    "karma_losers = df['karma'][df['requester_received_pizza'] == 0].describe()\n",
    "karma_comparison = pd.concat([karma_winners, karma_losers], axis=1)\n",
    "karma_comparison.columns = ['winners', 'losers']\n",
    "karma_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick analysis of the newly created \"karma\" features highlights the fact that people who received pizza have a median karma value of over 660, compared to those who did not with a median value of only 480. This large difference will likely have strong predictve power for our model.  As indicated in the code snippet below, roughly one-third of all observations who did not receive a pizza have a karma value below 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820 0.26920551543\n",
      "163 0.163983903421\n",
      "237 0.0778069599475\n",
      "81 0.0814889336016\n"
     ]
    }
   ],
   "source": [
    "low_losers= len(df[df['requester_received_pizza'] == 0][df['karma'] < 15])\n",
    "low_winners=len(df[df['requester_received_pizza'] == 1][df['karma'] < 15])\n",
    "print low_losers, low_losers/3046.\n",
    "print low_winners, low_winners/994.\n",
    "\n",
    "high_losers = len(df[df['requester_received_pizza'] == 0][df['karma'] > 5000])\n",
    "high_winners = len(df[df['requester_received_pizza'] == 1][df['karma'] > 5000])\n",
    "print high_losers, high_losers/3046.\n",
    "print high_winners, high_winners/994.\n",
    "\n",
    "df['karma_low'] = np.where(df['karma'] < 15, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Request length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winners</th>\n",
       "      <th>losers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>994.000000</td>\n",
       "      <td>3046.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>104.441650</td>\n",
       "      <td>84.278070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>77.989213</td>\n",
       "      <td>68.340357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>128.750000</td>\n",
       "      <td>102.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>828.000000</td>\n",
       "      <td>862.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          winners       losers\n",
       "count  994.000000  3046.000000\n",
       "mean   104.441650    84.278070\n",
       "std     77.989213    68.340357\n",
       "min      8.000000     4.000000\n",
       "25%     55.000000    44.000000\n",
       "50%     84.000000    67.000000\n",
       "75%    128.750000   102.000000\n",
       "max    828.000000   862.000000"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_winners = df['total_length'][df['requester_received_pizza'] == 1].describe()\n",
    "length_losers = df['total_length'][df['requester_received_pizza'] == 0].describe()\n",
    "length_comparison = pd.concat([length_winners, length_losers], axis=1)\n",
    "length_comparison.columns = ['winners', 'losers']\n",
    "length_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The analsyis here indicates that while not as dramatic a difference as the previous feature, there is a moderate difference in text length between winners and losers.  Apparently, a longer text (request) indicates more of an effort to explain the users particular situation, and is therefore more likely to be seen as sincere or plausible by the RAOP community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Extracting time-based features:\n",
    "#### The UTC time stamp is parsed and converted to human readable format. Also, the first half of the month is identified with a binary variable since people might be more money to use for donating a pizza earlier in the month. XXX time inconsistency, will consider if there's time left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['timestamp']=df['unix_timestamp_of_request_utc'].apply(lambda x:datetime.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "df['month']=df['unix_timestamp_of_request_utc'].apply(lambda x:int(datetime.datetime.fromtimestamp(int(x)).strftime('%m')))\n",
    "df['day']=df['unix_timestamp_of_request_utc'].apply(lambda x:int(datetime.datetime.fromtimestamp(int(x)).strftime('%d')))\n",
    "df['time']=df['unix_timestamp_of_request'].apply(lambda x:int(datetime.datetime.fromtimestamp(int(x)).strftime('%H')))\n",
    "df['first_half'] = np.where(df['day'] < 16, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Extracting gratitude and \"pay it forward\" sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create binary variables that show whether requester is grateful or willing to \"pay it forward\"\n",
    "df['requester_grateful'] = np.where(df['request_text_n_title'].str.contains('thanks' or 'advance' or 'guy'\\\n",
    "                    or 'reading' or 'anyone' or 'anything' or'story'or 'tonight'or 'favor'or'craving'), int(1), int(0))\n",
    "\n",
    "df['requester_payback'] = np.where(df['request_text_n_title'].str.contains('return' or 'pay it back'\\\n",
    "                                                                     or 'pay it forward' or 'favor'), int(1), int(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Extracting request narratives\n",
    "#### We extract the narrative out of each request. Based on our EDA, we picked 5 narratives: Money, Job, Student, Family and Craving. 5 new columns will be added for the narratives which are coded in binary 0 or 1.  The terms that specify each narrative are given in the narratives' dictionary.  We counted the total occurent of the words in each post, normalize this count by the word count in the post, and use median-threshold to determin whether the post fall into the respective narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define narrative categories\n",
    "narratives = {\n",
    "            'money':['money','now','broke','week','until','time',\n",
    "                      'last','day','when','today','tonight','paid',\n",
    "                      'next','first','night','night','after','tomorrow',\n",
    "                      'while','account','before','long','friday','rent',\n",
    "                      'buy','bank','still','bills','ago','cash','due',\n",
    "                      'soon','past','never','paycheck','check','spent',\n",
    "                      'year','years','poor','till','yesterday','morning',\n",
    "                      'dollars','financial','hour','bill','evening','credit',\n",
    "                      'budget','loan','bucks','deposit','dollar','current','payed'],\n",
    "             'job':['work','job','paycheck','unemployment','interviewed',\n",
    "                   'fired','employment','hired','hire'],\n",
    "             'student':['college','student','school','roommate','studying',\n",
    "                       'study','university','finals','semester','class','project',\n",
    "                       'dorm','tuition'],\n",
    "             'family': ['family','mom','wife','parents','mother','husband','dad','son',\n",
    "                     'daughter','father','parent','mum','children','starving','hungry'],\n",
    "             'craving': ['friend','girlfriend','birthday','boyfriend','celebrate',\n",
    "                      'party','game','games','movie','movies','date','drunk',\n",
    "                      'beer','celebrating','invited','drinks','crave','wasted','invited']\n",
    "            }\n",
    "\n",
    "# function to extract word count for each narrative from one text post, normalize by the word count of that post\n",
    "def single_extract(text):  \n",
    "    count = {'money':0.,\n",
    "            'job':0.,\n",
    "            'student':0.,\n",
    "            'family':0.,\n",
    "            'craving':0.}\n",
    "    words = text.split(' ')\n",
    "    length = 1./len(words)\n",
    "    for word in text.split(' '):\n",
    "        for i,k in narratives.items():\n",
    "            if word in k:\n",
    "                count[i] += length\n",
    "    return count.values()\n",
    "\n",
    "# Extract request_text_n_title field\n",
    "texts = df['request_text_n_title'].copy()\n",
    "\n",
    "#initialize count \n",
    "count =[]\n",
    "# return normalized count for each narrative from all requests\n",
    "for text in texts:\n",
    "    count.append(single_extract(text))\n",
    "\n",
    "# narrative dataframe\n",
    "narrative = pd.DataFrame(count)\n",
    "# set up median for using with the test set\n",
    "median_values = []\n",
    "\n",
    "#extract narrative field\n",
    "for i,k in enumerate(narratives.keys()):\n",
    "    median_values.append(np.median(narrative[i]))\n",
    "    narrative['narrative_'+k] = (narrative[i] > np.median(narrative[i])).astype(int)\n",
    "    narrative.drop([i],axis=1,inplace=True)\n",
    "\n",
    "# concatenate \n",
    "df = pd.concat([df,narrative],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the end, we added a total of 16 additional features, 3 continuous and 13 binary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4040 entries, 0 to 4039\n",
      "Data columns (total 32 columns):\n",
      "request_text                                          4040 non-null object\n",
      "request_title                                         4040 non-null object\n",
      "requester_account_age_in_days_at_request              4040 non-null float64\n",
      "requester_days_since_first_post_on_raop_at_request    4040 non-null float64\n",
      "requester_number_of_comments_at_request               4040 non-null int64\n",
      "requester_number_of_comments_in_raop_at_request       4040 non-null int64\n",
      "requester_number_of_posts_at_request                  4040 non-null int64\n",
      "requester_number_of_posts_on_raop_at_request          4040 non-null int64\n",
      "requester_number_of_subreddits_at_request             4040 non-null int64\n",
      "requester_received_pizza                              4040 non-null int64\n",
      "requester_subreddits_at_request                       4040 non-null object\n",
      "requester_upvotes_minus_downvotes_at_request          4040 non-null int64\n",
      "requester_upvotes_plus_downvotes_at_request           4040 non-null int64\n",
      "unix_timestamp_of_request                             4040 non-null int64\n",
      "unix_timestamp_of_request_utc                         4040 non-null int64\n",
      "request_text_n_title                                  4040 non-null object\n",
      "total_length                                          4040 non-null int64\n",
      "image_incl                                            4040 non-null int64\n",
      "karma                                                 4040 non-null float64\n",
      "karma_low                                             4040 non-null int64\n",
      "timestamp                                             4040 non-null object\n",
      "month                                                 4040 non-null int64\n",
      "day                                                   4040 non-null int64\n",
      "time                                                  4040 non-null int64\n",
      "first_half                                            4040 non-null int64\n",
      "requester_grateful                                    4040 non-null int64\n",
      "requester_payback                                     4040 non-null int64\n",
      "narrative_money                                       4040 non-null int64\n",
      "narrative_job                                         4040 non-null int64\n",
      "narrative_family                                      4040 non-null int64\n",
      "narrative_student                                     4040 non-null int64\n",
      "narrative_craving                                     4040 non-null int64\n",
      "dtypes: float64(3), int64(24), object(5)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To fit our model, we combined all engineered features into a single dataframe.  And after several iterations, decided to truncate some of the added features to improve predictive accuracy ('time' and 'karma' specifically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_incl</th>\n",
       "      <th>karma_low</th>\n",
       "      <th>time</th>\n",
       "      <th>first_half</th>\n",
       "      <th>requester_grateful</th>\n",
       "      <th>requester_payback</th>\n",
       "      <th>narrative_money</th>\n",
       "      <th>narrative_job</th>\n",
       "      <th>narrative_family</th>\n",
       "      <th>narrative_student</th>\n",
       "      <th>narrative_craving</th>\n",
       "      <th>total_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_incl  karma_low  time  first_half  requester_grateful  requester_payback  narrative_money  narrative_job  narrative_family  narrative_student  narrative_craving  total_length\n",
       "0           0          1    16           1                   0                  0                0              0                 1                  0                  0      0.009914\n",
       "1           0          0    23           0                   0                  0                1              0                 0                  0                  0      0.003443\n",
       "2           0          1    11           0                   0                  0                1              0                 1                  0                  1      0.009364\n",
       "3           0          0    12           1                   0                  0                0              0                 1                  0                  0      0.005370\n",
       "4           0          0    13           1                   0                  0                0              0                 0                  0                  1      0.015836"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_list = ['total_length']#, 'time', 'karma']\n",
    "binary_list = [\n",
    "                                               u'image_incl',\n",
    "                                                u'karma_low',\n",
    "                                            #        u'month',\n",
    "                                             #         u'day',\n",
    "                                                     u'time',\n",
    "                                               u'first_half',\n",
    "                                       u'requester_grateful',\n",
    "                                        u'requester_payback',\n",
    "                                          u'narrative_money',\n",
    "                                            u'narrative_job',\n",
    "                                         u'narrative_family',\n",
    "                                        u'narrative_student',\n",
    "                                        u'narrative_craving'\n",
    "             ]\n",
    "\n",
    "#create new DataFrame using previously defined \"numeric_features\" object to determine all columns in the DF\n",
    "\n",
    "numeric_features = df.copy().loc[:,continuous_list]\n",
    "numeric_features_norm = pd.DataFrame(data=preprocessing.normalize(numeric_features, axis=0),\\\n",
    "                                     columns=numeric_features.columns.values)  \n",
    "\n",
    "# combine to contious and binary\n",
    "numeric_features_norm = pd.concat([df[binary_list],numeric_features_norm],axis=1)\n",
    "numeric_features_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4040 entries, 0 to 4039\n",
      "Data columns (total 12 columns):\n",
      "image_incl            4040 non-null int64\n",
      "karma_low             4040 non-null int64\n",
      "time                  4040 non-null int64\n",
      "first_half            4040 non-null int64\n",
      "requester_grateful    4040 non-null int64\n",
      "requester_payback     4040 non-null int64\n",
      "narrative_money       4040 non-null int64\n",
      "narrative_job         4040 non-null int64\n",
      "narrative_family      4040 non-null int64\n",
      "narrative_student     4040 non-null int64\n",
      "narrative_craving     4040 non-null int64\n",
      "total_length          4040 non-null float64\n",
      "dtypes: float64(1), int64(11)\n",
      "memory usage: 410.3 KB\n"
     ]
    }
   ],
   "source": [
    "features = numeric_features_norm.copy()\n",
    "target = df['requester_received_pizza']\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Final Modeling\n",
    "#### We decided to create three final models.  \n",
    "* One model using just our engineered features (without unigram text)\n",
    "* One combined model using engineered features and unigram text\n",
    "* One model using predicted probabilities as model features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAAUBAMAAAAq47RJAAAAMFBMVEX///8AAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAv3aB7AAAAD3RSTlMAIma7zZnddlTvRIkQ\nMqvFy5UvAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAEuElEQVRYCZ1XQWwUVRj+Zna7O7szbZcSuHBg\nMHggHlzkYkzUNb0YAmET2As1ZImxMcHIcjAeNLIe5Mg2ITGB1GQvNvHEAlZiGuMcENJg6F682rUH\njYmCAkKxq+v//2+6nXnzSur8yfvy3v9/833zd96+mQKWDxn4H7HHzLV8c96Q3URhwkAF7JIxbfla\nWngC26lCY9fCi8COoy8D3139JgTtGllatQNdjJldSObWB8eZ9kM6BasMa+FaE2LiTT3XhQB+Mt0J\n37VmJzyG12RYVbR8r4dv23gdN0sKdClnFrgD9294Fb0ka5KqYqxJ8zeRSiFbwg4495VJtumdhABu\niLwOSTvhEdhVGflVjNfHAhRm7Bm4FQFd5M6JPvAbcBG4rtd4TVKOD6sC5C8glcKzwNkmHimTfcCX\nEIDbMPgZ7IRHkKOdRKN4CUuVQh1j/2R6cPoCCaE8NfUpcK+LxUSNEiTjluHVgYlJpFI4CXzR9h4q\nk1eAViAAb8bgZ7ATHsFOovMgBT/bp6bGeyiuCkg2CtzUkYCbyraj+XBOMsUHgesDnUmkUbDqrETb\nT0z+BS43BYCDXNDCZCe8g3iPqDzgnWLM9ZeqKD4W4HUsuCmK0wFc2rSJYJl796/yaTWJNAp5Ub3Z\nYeXT3/9FTX3I0ADmE2bqrnU74c3jBaLzsL56gy+83NndweiaAK9joZoqrtFtV2IFtWAZd3CIziRM\nIo1CxieFXW8FhMU1i2yWrzBQk3sppYfJTnh78TZxeQBftwnexe4qN8XAyViopnI92iL1WEEtWOb8\n3JM2OtxUCoVck4XcSwS5nkUPafkKAzV1jgtamOyEdw6swAMozAIjVbVvnrZ5asQd/UMuiQPJ2D2c\nnXW61FQahUJbBC8GQA0e9TPcfstxJ1mZ7IS3vN6UVUJmVfbpOJ1+fFA4tNRCnpRs/M2aKpQwunob\n1FQaBW5qG/BqE2xCZ0SLD4pWk56Wdie8pKYSdmFT/BBpjPe5KaeKbZky7L5AQkia+gT0ByjWE0WR\nWaL0yvm7d5/8kkaBt98g4KbY5ADwfCAAzG3RTnhz6wdFoYzcn/x1c8aewUhFICHETY1WkS/BriSK\nct7Qnw77qSQv35Ek6ekKfFBcoIOvJCb7gGP88j1GepscFAm78KCYp0to2D5aDevUwok6PbfFrgC9\nBePBt7RzYf5n+u3Rjxd6nWSKh2A3qPSAn9tiV2dAU1iqxAxc2nTPwHmsTHJN7zMIAFNbslM8IvMD\n4/F57SVkB4NBHbff/x0KpgMqbMTIrw9XcGQweARku5yeDhiHwTI/Th0nvDZY2ZJC7p3hxTzhTV2s\nHW0rE+/6R/RBywAc5vJ0wDgMg53iHV7/TBpSoxOnFF1F5/Q2p9Dq9N2ih8aIlJXCx5EMTekzyRhW\nndOamMFOeAThB61RyzVmOak+aLU6S2mhMSJVpdCJZGhKH7TGcBuc1sRMdsxjMv+ueRjiliEnqfBf\nD72elNEZQ0Gl4MnNDpPIlDbm0dkNWehiSTvhMWwPR1QknPuGnKRCc73OUvHw48uNlVKwNxIys8pa\nIlyq2/e1YtJOeAyWrwZNtxx7zEzLN+cN2U0UJgxUOprND9DyNbbwCP4DA4bDiw3Q60gAAAAASUVO\nRK5CYII=\n",
      "text/latex": [
       "$$\\left ( \\left ( 3200, \\quad 12\\right ), \\quad \\left ( 840, \\quad 12\\right ), \\quad \\left ( 3200\\right ), \\quad \\left ( 840\\right )\\right )$$"
      ],
      "text/plain": [
       "((3200, 12), (840, 12), (3200,), (840,))"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = features.values, target.copy()\n",
    "#shuffle = np.random.permutation(len(X))\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "Xtrain, Xdev, ytrain, ydev = X[:3200], X[3200:], y[:3200], y[3200:]\n",
    "Xtrain.shape, Xdev.shape, ytrain.shape, ydev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Model One: Engineered Features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C-value: 21.0534210526\n",
      "Best AUC score based on metrics.roc_auc_score = 0.5941\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "c_values = np.linspace(.001, 100, 20)\n",
    "for c in c_values:\n",
    "    lr_8 = LogisticRegression(C=c, class_weight='balanced', n_jobs=-1).fit(Xtrain, ytrain)\n",
    "    predictions = lr_8.predict(Xdev)\n",
    "    scores.append(round(roc_auc_score(ydev, predictions, average='weighted'), 4))\n",
    "print \"Best C-value: {}\".format(c_values[np.argmax(scores)])\n",
    "print 'Best AUC score based on metrics.roc_auc_score = {}'.format(max(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Model Two: Engineered Features combined with Unigram text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4040, 399)\n",
      "Best C-value: 3.40986106044\n",
      "Best ROC AUC Score: 0.6327\n"
     ]
    }
   ],
   "source": [
    "#Initialize our vectorizer using hyperparameters from previous models\n",
    "vec = TfidfVectorizer(stop_words='english',sublinear_tf=1, max_features=best_max_feature)\n",
    "train_matrix = vec.fit_transform(train_data)\n",
    "\n",
    "#Transform our text features\n",
    "text_features = df['request_text_n_title'].values.copy()\n",
    "text_transformed = vec.transform(text_features)\n",
    "\n",
    "# Concatenate egineered numeric_features with vectorized text features \n",
    "combined_features = np.append(features.values, text_transformed.toarray(), axis = 1)\n",
    "print combined_features.shape\n",
    "\n",
    "#prep data for modeling\n",
    "X, y = combined_features, target.copy()\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "Xtrain, Xdev, ytrain, ydev = X[:3200], X[3200:], y[:3200], y[3200:]\n",
    "Xtrain.shape, Xdev.shape, ytrain.shape, ydev.shape\n",
    "\n",
    "#Fit logistic model to data\n",
    "c_values = np.logspace(.0001, 2, 200)\n",
    "c_scores = []\n",
    "for value in c_values:\n",
    "    lr = LogisticRegression(C=value, n_jobs=-1, class_weight='balanced', penalty='l2')\n",
    "    lr.fit(Xtrain, ytrain)\n",
    "    predictions = lr.predict(Xdev)\n",
    "    c_scores.append(round(metrics.roc_auc_score(ydev, predictions, average = 'weighted'), 4))\n",
    "    \n",
    "best_C_value = c_values[np.argmax(c_scores)]\n",
    "print \"Best C-value: {}\".format(best_C_value)\n",
    "print \"Best ROC AUC Score: {}\".format(c_scores[np.argmax(c_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Three: Unigrams predict_probability combined with engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C-value: 26.7401797071\n",
      "Best ROC AUC Score: 0.6137\n"
     ]
    }
   ],
   "source": [
    "#fit logistic classifier to training data\n",
    "vec = TfidfVectorizer(stop_words='english',sublinear_tf=1, max_features=best_max_feature)\n",
    "train_matrix = vec.fit_transform(train_data)\n",
    "\n",
    "\n",
    "lr1 = LogisticRegression(C=best_c_value, n_jobs=-1).fit(train_matrix, train_labels)\n",
    "\n",
    "text_features = df['request_text_n_title'].values.copy()\n",
    "\n",
    "text_transformed = vec.transform(text_features)\n",
    "# create pizza_predict field for all records based on features array\n",
    "pizza_predict = lr1.predict_proba(text_transformed)[:,1][:, np.newaxis] \n",
    "\n",
    "# Concatenate numeric_features with pizza_predict to create pizza_predict + numeric features in ens_features\n",
    "combined_features = np.append(features.values, pizza_predict, axis = 1)\n",
    "\n",
    "X, y = combined_features, target.copy()\n",
    "#shuffle = np.random.permutation(len(X))\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "Xtrain, Xdev, ytrain, ydev = X[:3200], X[3200:], y[:3200], y[3200:]\n",
    "Xtrain.shape, Xdev.shape, ytrain.shape, ydev.shape\n",
    "\n",
    "#####\n",
    "c_values = np.logspace(.0001, 2, 200)\n",
    "c_scores = []\n",
    "for value in c_values:\n",
    "    lr = LogisticRegression(C=value, n_jobs=-1, class_weight='balanced', penalty='l2')\n",
    "    lr.fit(Xtrain, ytrain)\n",
    "    predictions = lr.predict(Xdev)\n",
    "    c_scores.append(round(metrics.roc_auc_score(ydev, predictions, average = 'weighted'), 4))\n",
    "    \n",
    "best_C_value = c_values[np.argmax(c_scores)]\n",
    "print \"Best C-value: {}\".format(best_C_value)\n",
    "print \"Best ROC AUC Score: {}\".format(c_scores[np.argmax(c_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Results and Analysis\n",
    "#### Our best model ended up being model Two which was a combination of engineered features and unigram text.  This result is not surprising, especially considering the amount of work required to engineer the given features.  We submitted different variants of model two to the Kaggle website with varying degrees of success. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### FOR SUBMISSION WITH FINAL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Data shape: \t(4040,)\n",
      "Training Labels shape: \t(4040,)\n",
      "Test data shape: \t(1631,)\n",
      "(1631, 393)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 393 features per sample; expecting 387",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-395-e03fa0ae3350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#fit Logistic Regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_test_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# print type(predictions_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/americanthinker/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/americanthinker/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 317\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 393 features per sample; expecting 387"
     ]
    }
   ],
   "source": [
    "# SUBMISSION OF ABOVE TO KAGGLE\n",
    "\n",
    "df_test = pd.read_json('test.json')\n",
    "\n",
    "df_test['request_text_n_title'] = (df_test['request_title'] + ' ' + df_test['request_text_edit_aware'])\n",
    "df_test['request_text_n_title'] = [ text.split(\" \",1)[1].lower() for text in df_test['request_text_n_title']]\n",
    "# print df_test['request_text_n_title'].head()\n",
    "\n",
    "#create test set\n",
    "test_features = df_test['request_text_n_title'].values # text features\n",
    "\n",
    "#create new training set using all 4040 training samples\n",
    "train_features = df['request_text_n_title'].values\n",
    "train_labels = df['requester_received_pizza'].values\n",
    "\n",
    "#shuffle our data to ensure randomization\n",
    "shuffle = np.random.permutation(len(train_features))\n",
    "train_features, train_labels = train_features[shuffle], train_labels[shuffle]\n",
    "\n",
    "#check to ensure we've got the right datasets\n",
    "print '\\n\\nTraining Data shape: \\t{}'.format(train_features.shape)\n",
    "print 'Training Labels shape: \\t{}'.format(train_labels.shape)\n",
    "print 'Test data shape: \\t{}'.format(test_features.shape)\n",
    "\n",
    "#transform raw data into Tfidf vector\n",
    "vec = TfidfVectorizer(stop_words='english',sublinear_tf=1, max_features=best_max_feature)\n",
    "train_matrix = vec.fit_transform(train_features)\n",
    "\n",
    "#Transform our text features\n",
    "text_features = df_test['request_text_n_title'].values.copy()\n",
    "text_transformed = vec.transform(text_features)\n",
    "\n",
    "#Create engineered features for test set\n",
    "df_test['karma'] = df['requester_account_age_in_days_at_request'] + df['requester_days_since_first_post_on_raop_at_request']\\\n",
    "+ df['requester_number_of_comments_at_request'] + df['requester_number_of_comments_in_raop_at_request'] + \\\n",
    "df['requester_number_of_posts_at_request'] + df['requester_number_of_posts_on_raop_at_request'] + \\\n",
    "df['requester_number_of_subreddits_at_request'] + df['requester_upvotes_minus_downvotes_at_request']\n",
    "\n",
    "df_test['requester_grateful'] = np.where(df_test['request_text_n_title'].str.contains('thanks' or 'advance' or 'guy'\\\n",
    "                    or 'reading' or 'anyone' or 'anything' or'story'or 'tonight'or 'favor'or'craving'), int(1), int(0))\n",
    "\n",
    "df_test['requester_payback'] = np.where(df_test['request_text_n_title'].str.contains('return' or 'pay it back'\\\n",
    "                                                                     or 'pay it forward' or 'favor'), int(1), int(0))\n",
    "df_test['karma_low'] = np.where(df_test['karma'] <=15, 1, 0)\n",
    "df_test['image_incl'] = np.where(df_test['request_text_n_title'].str.contains(\"imgur\"), int(1), int(0))\n",
    "df_test['total_length'] =df_test['request_text_n_title'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "#create matrix of features\n",
    "test_features = df_test.loc[:, 'karma':'total_length'].values\n",
    "combined_test_features = np.append(test_features, text_transformed.toarray(), axis = 1)\n",
    "print combined_test_features.shape\n",
    "\n",
    "#fit Logistic Regression model\n",
    "lr= LogisticRegression(n_jobs=-1, class_weight='balanced').fit(train_matrix, train_labels)\n",
    "test_predictions = lr.predict(combined_test_features)[:, np.newaxis]\n",
    "\n",
    "# print type(predictions_test)\n",
    "sub_5 = np.append(df_test['request_id'].values[:, np.newaxis], test_predictions, axis = 1)\n",
    "sub_5_df = pd.DataFrame(data=sub_4, columns=['request_id', 'requester_received_pizza'])  # 1st row as the column names\n",
    "sub_5_df.to_csv(\"submission_5.csv\", sep=',', header=True,  mode='w', index=0)\n",
    "\n",
    "print \"Kaggle submission 1, result AUC = 0.53065\"\n",
    "print \"Kaggle submission 2, result AUC = 0.59735\"\n",
    "print \"Kaggle submission 3, result AUC = 0.57728\"\n",
    "\n",
    "# predictions_test.sum()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1631 entries, 0 to 1630\n",
      "Data columns (total 24 columns):\n",
      "giver_username_if_known                               1631 non-null object\n",
      "request_id                                            1631 non-null object\n",
      "request_text_edit_aware                               1631 non-null object\n",
      "request_title                                         1631 non-null object\n",
      "requester_account_age_in_days_at_request              1631 non-null float64\n",
      "requester_days_since_first_post_on_raop_at_request    1631 non-null float64\n",
      "requester_number_of_comments_at_request               1631 non-null int64\n",
      "requester_number_of_comments_in_raop_at_request       1631 non-null int64\n",
      "requester_number_of_posts_at_request                  1631 non-null int64\n",
      "requester_number_of_posts_on_raop_at_request          1631 non-null int64\n",
      "requester_number_of_subreddits_at_request             1631 non-null int64\n",
      "requester_subreddits_at_request                       1631 non-null object\n",
      "requester_upvotes_minus_downvotes_at_request          1631 non-null int64\n",
      "requester_upvotes_plus_downvotes_at_request           1631 non-null int64\n",
      "requester_username                                    1631 non-null object\n",
      "unix_timestamp_of_request                             1631 non-null int64\n",
      "unix_timestamp_of_request_utc                         1631 non-null int64\n",
      "request_text_n_title                                  1631 non-null object\n",
      "karma                                                 1631 non-null float64\n",
      "requester_grateful                                    1631 non-null int64\n",
      "requester_payback                                     1631 non-null int64\n",
      "karma_low                                             1631 non-null int64\n",
      "image_incl                                            1631 non-null int64\n",
      "total_length                                          1631 non-null int64\n",
      "dtypes: float64(3), int64(14), object(7)\n",
      "memory usage: 318.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# function process will run tfidVectorizer and create engineer features\n",
    "def process(filename,max_feature_length,median_values,train=True):\n",
    "\n",
    "    df = pd.read_json(filename)\n",
    "    \n",
    "    \n",
    "    if train==True:\n",
    "        # binarize sucess\n",
    "        # combine title and text\n",
    "        df['request_text_n_title'] = (df['request_title'] + ' ' + df['request_text'])\n",
    "        \n",
    "    else:\n",
    "        df['request_text_n_title'] = (df['request_title'] + ' ' + df['request_text_edit_aware'])\n",
    "    \n",
    "    \n",
    "    df['request_text_n_title'] = [ text.split(\" \",1)[1].lower() for text in df['request_text_n_title']]\n",
    "    #total length\n",
    "    df['total_length'] =df['request_text_n_title'].apply(lambda x: len(x.split(' ')))\n",
    "    \n",
    "    #get engineer features\n",
    "    \n",
    "    # image included?\n",
    "    df['image_incl'] = np.where(df['request_text_n_title'].str.contains(\"imgur\"), int(1), int(0))\n",
    "    #Karma\n",
    "    df['karma'] = df['requester_account_age_in_days_at_request']\\\n",
    "                 + df['requester_days_since_first_post_on_raop_at_request']\\\n",
    "                 + df['requester_number_of_comments_at_request'] + df['requester_number_of_comments_in_raop_at_request'] + \\\n",
    "                df['requester_number_of_posts_at_request'] + df['requester_number_of_posts_on_raop_at_request'] + \\\n",
    "                df['requester_number_of_subreddits_at_request'] + df['requester_upvotes_minus_downvotes_at_request']\n",
    "    \n",
    "    \n",
    "    df['karma_low'] = np.where(df['karma'] < 15, 1, 0)\n",
    "    \n",
    "    \n",
    "    # time\n",
    "    df['day']=df['unix_timestamp_of_request_utc'].apply(lambda x:int(datetime.datetime.fromtimestamp(int(x)).strftime('%d')))\n",
    "    df['time']=df['unix_timestamp_of_request'].apply(lambda x:int(datetime.datetime.fromtimestamp(int(x)).strftime('%H')))\n",
    "    df['first_half'] = np.where(df['day'] < 16, 1, 0)\n",
    "    \n",
    "    # requester's attitude\n",
    "    df['requester_grateful'] = np.where(df['request_text_n_title'].\n",
    "                               str.contains('thanks' or 'advance' or 'guy'\\\n",
    "                                            or 'reading' or 'anyone' or 'anything'\\\n",
    "                                           'story'or 'tonight'or 'favor'or'craving'), \n",
    "                                            int(1), int(0))\n",
    "\n",
    "    df['requester_payback'] = np.where(df['request_text_n_title'].\n",
    "                               str.contains('return' or 'pay it back' or 'pay it forward' or 'favor'), int(1), int(0))\n",
    "\n",
    "    #narrative\n",
    "    # Define narrative categories\n",
    "    narratives = {\n",
    "                'money':['money','now','broke','week','until','time',\n",
    "                          'last','day','when','today','tonight','paid',\n",
    "                          'next','first','night','night','after','tomorrow',\n",
    "                          'while','account','before','long','friday','rent',\n",
    "                          'buy','bank','still','bills','ago','cash','due',\n",
    "                          'soon','past','never','paycheck','check','spent',\n",
    "                          'year','years','poor','till','yesterday','morning',\n",
    "                          'dollars','financial','hour','bill','evening','credit',\n",
    "                          'budget','loan','bucks','deposit','dollar','current','payed'],\n",
    "                 'job':['work','job','paycheck','unemployment','interviewed',\n",
    "                       'fired','employment','hired','hire'],\n",
    "                 'student':['college','student','school','roommate','studying',\n",
    "                           'study','university','finals','semester','class','project',\n",
    "                           'dorm','tuition'],\n",
    "                 'family': ['family','mom','wife','parents','mother','husband','dad','son',\n",
    "                         'daughter','father','parent','mum','children','starving','hungry'],\n",
    "                 'craving': ['friend','girlfriend','birthday','boyfriend','celebrate',\n",
    "                          'party','game','games','movie','movies','date','drunk',\n",
    "                          'beer','celebrating','invited','drinks','crave','wasted','invited']\n",
    "                }\n",
    "\n",
    "    # function to extract word count for each narrative from one text post\n",
    "    # normalize by the word count of that post\n",
    "    def single_extract(text):  \n",
    "        count = {'money':0.,\n",
    "                'job':0.,\n",
    "                'student':0.,\n",
    "                'family':0.,\n",
    "                'craving':0.}\n",
    "        words = text.split(' ')\n",
    "        length = 1./len(words)\n",
    "        for word in text.split(' '):\n",
    "            for i,k in narratives.items():\n",
    "                if word in k:\n",
    "                    count[i] += length\n",
    "        return count.values()\n",
    "\n",
    "    # Extract request_text_n_title field\n",
    "    texts = df['request_text_n_title'].copy()\n",
    "\n",
    "    #initialize count \n",
    "    count =[]\n",
    "    # return normalized count for each narrative from all requests\n",
    "    for text in texts:\n",
    "        count.append(single_extract(text))\n",
    "\n",
    "    # narrative dataframe\n",
    "    narrative = pd.DataFrame(count)\n",
    "    # set up median for using with the test set\n",
    "\n",
    "    #extract narrative field\n",
    "    for i,k in enumerate(narratives.keys()):\n",
    "        narrative['narrative_'+k] = (narrative[i] > median_values[i]).astype(int)\n",
    "        narrative.drop([i],axis=1,inplace=True)\n",
    "\n",
    "    # concatenate \n",
    "    df = pd.concat([df,narrative],axis=1)\n",
    "    \n",
    "    \n",
    "    continuous_list = ['karma','total_length', 'time']\n",
    "    binary_list = ['requester_grateful','requester_payback',\n",
    "              'narrative_money','narrative_job',\n",
    "              'narrative_family', 'narrative_student', 'narrative_craving', 'narrative_money', 'first_half',\n",
    "              'image_incl', 'karma_low']\n",
    "    \n",
    "    #create new DataFrame using previously defined \"numeric_features\" object to determine all columns in the DF\n",
    "\n",
    "    continuous_features = df.copy().loc[:,continuous_list]\n",
    "    continuous_features_norm = pd.DataFrame(data=preprocessing.normalize(continuous_features, axis=0),\\\n",
    "                                         columns=numeric_features.columns.values)\n",
    "    \n",
    "    engineered_features = pd.concat([df[binary_list],continuous_features_norm],axis=1)\n",
    "\n",
    "    \n",
    "    if train==True:\n",
    "        # binarize sucess\n",
    "        # combine title and text\n",
    "        df['requester_received_pizza'] = np.where(df['requester_received_pizza'] == True, 1, 0)\n",
    "            # Get vectorized fields\n",
    "        vec = TfidfVectorizer(stop_words='english',sublinear_tf=1, max_features=max_feature_length)\n",
    "        text_features = df['request_text_n_title'].values.copy()\n",
    "        vectorized_matrix = vec.fit_transform(text_features)\n",
    "        target = df['requester_received_pizza']\n",
    "        return vec, vectorized_matrix, engineered_features, target\n",
    "    else:\n",
    "\n",
    "        text_features = df['request_text_n_title'].values.copy()\n",
    "        request_id = df['request_id']\n",
    "        return request_id,text_features, engineered_features\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec, train_tfid_matrix, train_engr_features,train_target = process('train.json',best_max_feature,median_values)\n",
    "\n",
    "# Building model 1 on pure logistic regression\n",
    "lr1 = LogisticRegression( n_jobs=-1,class_weight='balanced').fit(train_tfid_matrix, train_target)\n",
    "\n",
    "# Model 2 on combined tfid_matrix and engr features\n",
    "combined_features = np.append(train_engr_features.values, train_tfid_matrix.toarray(), axis = 1)\n",
    "\n",
    "lr2 = LogisticRegression( n_jobs=-1,class_weight='balanced').fit(combined_features, train_target)\n",
    "\n",
    "# get test\n",
    "request_id,test_text_features, test_engr_features = process('test.json',best_max_feature,median_values,train=False)\n",
    "\n",
    "test_tfid_matrix = vec.transform(test_text_features)\n",
    "\n",
    "combined_test_features = np.append(test_engr_features.values,test_tfid_matrix.toarray(),axis=1)\n",
    "\n",
    "#  predict\n",
    "\n",
    "predict1 = lr1.predict(test_tfid_matrix)[:,np.newaxis]\n",
    "predict2 = lr2.predict(combined_test_features)[:,np.newaxis]\n",
    "\n",
    "# submissions\n",
    "\n",
    "# print type(predictions_test)\n",
    "sub_1 = np.append(request_id.values[:, np.newaxis], predict1, axis = 1)\n",
    "sub_1_df = pd.DataFrame(data=sub_1, columns=['request_id', 'requester_received_pizza'])  # 1st row as the column names\n",
    "sub_1_df.to_csv(\"submission_1.csv\", sep=',', header=True,  mode='w', index=0)\n",
    "\n",
    "sub_2 = np.append(request_id.values[:, np.newaxis], predict2, axis = 1)\n",
    "sub_2_df = pd.DataFrame(data=sub_2, columns=['request_id', 'requester_received_pizza'])  # 1st row as the column names\n",
    "sub_2_df.to_csv(\"submission_2.csv\", sep=',', header=True,  mode='w', index=0)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
